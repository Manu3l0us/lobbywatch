#!/usr/bin/python3
# -*- coding: utf-8 -*-


# A script that imports PDFs that are on the site of the government that
# indicate which member of the two Swiss parliaments have which guests
# on their guest list.

# Since the information is only provided as PDF documents that are not easily
# machine-readable, this script translates the PDF into a JSON document hat can
# then be used for further automation.

# Created by Markus Roth in February 2017 (maroth@gmail.com)
# Licenced via Affero GPL v3

import requests
import csv
import json
import os
from subprocess import call
from collections import defaultdict


# read file from url while respecting redirects and accepting cookies
def get_url(url, filename):
    with open(filename, "wb") as target_file:
        r = requests.get(url)
        r = requests.get(url, cookies=r.cookies)
        target_file.write(r.content)


# read the csv generated by tabula and get rid of empty rows and headers
def cleanup_file(filename):
    guests = defaultdict(list)
    reader = csv.reader(open(filename, encoding="utf-8"))
    for row in reader:
        if not is_header(row) and not is_empty(row):
            if len(row[0].strip()) > 0:
                member_of_parlament = row[0]
            guest_name = row[1]
            guest_function = row[2]
            if len(guest_name.strip()) > 0:
                guests[clean_string(member_of_parlament)].append(
                    (clean_string(guest_name), clean_string(guest_function)))
    return guests


# remove invalid characters from cell entries
def clean_string(s):
    return s.replace("\n", " ")


# is this table row a header row?
def is_header(row):
    if len(row) < 2:
        return True
    header_words = ["Partito", "Cantone", "Consigliere", "Fonction",
                    "Conseiller", "Funzionenktion", "Funktion",
                    "Funzione", "Name", "Partei / Kanton", "Funzionenktion,",
                    "Conseiller/,", "Parti / Canton"]

    return any(header_word in row_entry
               for header_word in header_words
               for row_entry in row)


# is this table row empty?
def is_empty(row):
    return all(len(entry.strip()) == 0 for entry in row)


# write member of parliament and guests to json file
def write_to_json(guests, filename):
    data = defaultdict(list)
    for member_of_parlament, guests in guests.items():
        guest_list = []
        for guest, function in guests:
            guest_list.append({"name": guest, "function": function})
        data[member_of_parlament] = guest_list
    with open(filename, "wb") as json_file:
        contents = json.dumps(data, indent=4, separators=(',', ': '), ensure_ascii=False).encode("utf-8")
        json_file.write(contents)


# download a pdf containing the guest lists of members of parlament in a table
# then parse the file into json and save the json files to disk
def scrape_pdf(url, filename):
    print("\ndownloading " + url)
    get_url(url, "file.pdf")

    print("removing first page of PDF...")
    call(["pdftk", "file.pdf", "cat", "2-end", "output", "file-stripped.pdf"])

    print("parsing PDF...")
    # hide useless errors by piping stdout to dev/null
    FNULL = open(os.devnull, "w")
    call(["java", "-jar", "tabula-0.9.2-jar-with-dependencies.jar",
         "file-stripped.pdf", "--pages", "all", "-o", "data.csv"],
         stdout=FNULL, stderr=FNULL)

    print("cleaning up parsed data...")
    guests = cleanup_file("data.csv")

    print("writing " + filename + "...")
    write_to_json(guests, filename)

    print("cleaning up...")
    os.remove("file.pdf")
    os.remove("file-stripped.pdf")
    os.remove("data.csv")

    print("done!")


# scrape the nationalrat and ständerat guest lists and write them to
# structured JSON files
def scrape():
    root = "https://www.parlament.ch/centers/documents/de/"

    #scrape nationalrat
    scrape_pdf(root +
               "zutrittsberechtigte-nr.pdf",
               "zutrittsberechtige-nr.json")

    #scrape ständerat
    scrape_pdf(root +
               "zutrittsberechtigte-sr.pdf",
               "zutrittsberechtigte-sr.json")


#main method
if __name__ == "__main__":
    scrape()



